# Maloka Airflow Pipelines

Este reposit√≥rio cont√©m as DAGs e utilit√°rios para constru√ß√£o de pipelines de ingest√£o e transforma√ß√£o de dados no Apache Airflow, usando uma arquitetura em camadas (transient ‚Üí bronze ‚Üí silver) para m√∫ltiplas empresas.

---

## üìÅ Estrutura de Pastas

```text
airflow/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py          # Configura√ß√£o global (nome do bucket)
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ s3.py                # Cliente S3 com conex√£o e upload/download
‚îÇ   ‚îî‚îÄ‚îÄ layers.py            # Classes de camada (Transient, Bronze, Silver)
‚îî‚îÄ‚îÄ dags/
    ‚îî‚îÄ‚îÄ add/                 # Pipeline "add" (exemplo); pode replicar para outras empresas
        ‚îî‚îÄ‚îÄ dag_add_pipeline.py  # DAG √∫nica com TaskGroup por camada

‚îú‚îÄ‚îÄ requirements.txt         # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md                # Este arquivo
```

---

## ‚öôÔ∏è Pr√©‚ÄëRequisitos

1. **Sistema Operacional**: Ubuntu 24.04 ou similar
2. **Python**: >= 3.12
3. **Apache Airflow**: >= 2.7.1
4. **PostgreSQL**: para metastore (recomendado) ou SQLite (apenas dev)
5. **AWS Credentials**: Usu√°rio IAM com permiss√£o S3
6. **GitHub Actions**: configurado no CI/CD

---

## üöÄ Instala√ß√£o e Setup

### 1. Clonar o reposit√≥rio

```bash
git clone <URL_DO_REPO> ~/airflow/dags
cd ~/airflow
```

### 2. Configurar o virtualenv

```bash
python3 -m venv airflow-venv
source airflow-venv/bin/activate
pip install --upgrade pip
pip install -r dags/requirements.txt
```

### 3. Configurar o Metastore

> **Recomendado:** PostgreSQL + LocalExecutor

```bash
# Instalar e configurar PostgreSQL
sudo apt update && sudo apt install -y postgresql libpq-dev
sudo -u postgres createuser airflow --createdb --no-superuser
sudo -u postgres psql -c "ALTER USER airflow WITH PASSWORD 'SUA_SENHA';"
sudo -u postgres createdb airflow --owner airflow

# Ajustar airflow.cfg em ~/airflow/airflow.cfg
# sql_alchemy_conn = postgresql+psycopg2://airflow:SUA_SENHA@localhost:5432/airflow
# executor = LocalExecutor
# load_examples = False

export AIRFLOW_HOME=~/airflow
airflow db init
```  

> **Alternativa (dev):** SQLite (paliativo)
```ini
# em airflow.cfg:
# executor = SequentialExecutor
# [scheduler]
# parsing_processes = 1
# max_threads = 1
```  

### 4. Criar Usu√°rio Admin (RBAC)

```bash
airflow users create \
  --username admin \
  --firstname Maloka \
  --lastname Admin \
  --role Admin \
  --email admin@maloka.ai \
  --password youpass
```

### 5. Conex√£o S3 no Airflow

- Acesse **Admin ‚Üí Connections** na UI
- Crie conex√£o:
  - Conn¬†ID: `s3-conn-add`
  - Conn¬†Type: **Amazon Web Services**
  - Login: `<AWS_ACCESS_KEY_ID>`
  - Password: `<AWS_SECRET_ACCESS_KEY>`
  - Extra JSON:
    ```json
    {"region_name":"us-east-1"}
    ```

---

## üìú Configura√ß√£o Global

```python
# config/settings.py
BUCKET_NAME = "malokaai"
```

---

## üõ†Ô∏è Pipelines

### DAG: `add_pipeline`
- ID: `add_pipeline`
- Schedule: `@daily`
- Camadas:
  1. **transient**: gera CSV e faz upload em `s3://malokaai/add/transient/`
  2. **bronze**: l√™ CSV, adiciona `DATA_EXTRACAO`, converte para Parquet em `s3://malokaai/add/bronze/`
  3. **silver**: l√™ Parquet, gera `BK_COLUMNS`, limpa dados, grava em `s3://malokaai/add/silver/`

> **Observa√ß√£o**: para criar pipelines de outras empresas, duplique a DAG e ajuste:
> ```python
> AWS_CONN_ID    = "s3-conn-<empresa>"
> COMPANY_PREFIX = "<empresa>/"
> ```

---

## üì• Execu√ß√£o Manual

**Iniciar Airflow** (dentro do venv):
```bash
export AIRFLOW_HOME=~/airflow
airflow webserver --port 8080 --daemon
airflow scheduler --daemon
```
Acesse `http://<IP_OU_HOST>/airflow` e fa√ßa login com `SeuUSer`.

**Trigger DAG** no UI ou CLI:
```bash
airflow dags trigger add_pipeline
```  

---

## ‚öôÔ∏è CI/CD (GitHub Actions)

```yaml
name: Deploy Airflow DAGs to EC2
on: [push]
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy via SSH
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.HOST_DNS }}
          username: ${{ secrets.USERNAME }}
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            cd /home/ubuntu/airflow/dags
            git fetch origin main && git reset --hard origin/main
            cd /home/ubuntu
            source airflow-venv/bin/activate
            pip install --upgrade pip
            pip install -r airflow/dags/requirements.txt
            # reiniciar Airflow
            pkill -f "airflow webserver"; airflow webserver --port 8080 --daemon
            pkill -f "airflow scheduler"; airflow scheduler --daemon
```

---

## üéØ Boas Pr√°ticas

- **Modularize** seu c√≥digo em `utils/` e `config/` para reuso.
- **Use TaskGroup** para organizar visualmente camadas na DAG.
- **Evite download local** se seus arquivos couberem em mem√≥ria.
- **Use Postgres + LocalExecutor** em produ√ß√£o.
- **Gerencie processos** via systemd para alta disponibilidade.
- **Versione** suas DAGs e requirements via Git.

---

*Maloka Data Engineering*